# Gemma3 RAG KasenSabo MVP Configuration

# Data Paths
data:
  knowledge_base: "data/kasensabo_knowledge_base"
  questions: "questions/bench_questions_200.json"
  
# Index Configuration
index:
  path: "index/faiss_index"
  chunk_size: 512
  chunk_overlap: 50
  
# Embedding Model
embedding:
  model_name: "intfloat/multilingual-e5-base"  # 日本語対応・バランス型（1.1GB）
  device: "cuda" # if GPU available
  
# LLM Models (Ollama)
models:
  - name: "gemma:2b-instruct-q4_K_M"
    quantization: "INT4"
    alias: "gemma3-int4"
  - name: "gemma:2b-instruct-q8_0"
    quantization: "INT8"
    alias: "gemma3-int8"
    
# RAG Parameters
rag:
  temperature: 0.1
  top_k: 5
  max_tokens: 512
  similarity_top_k: 3
  
# Benchmark Configuration
benchmark:
  batch_size: 10
  save_interval: 50
  output_dir: "results"
  
# Evaluation Metrics
evaluation:
  metrics:
    - "exact_match"
    - "f1_score"
    - "bleu"
    - "rouge"
    - "response_time"
    - "memory_usage"
