"""
Post Evaluation Script - æ—¢å­˜ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã«è©•ä¾¡æŒ‡æ¨™ã‚’è¿½åŠ 
å‚ç…§å›ç­”ã‚’æ‰‹å‹•ã§å…¥åŠ›ã—ã¦ã€EM/F1/BLEU/ROUGEã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
"""

import json
import sys
from pathlib import Path
from typing import Dict, List
from evaluate import RAGEvaluator

def load_benchmark_results(filepath: str) -> Dict:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®èª­ã¿è¾¼ã¿"""
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)

def save_evaluation_results(filepath: str, results: Dict):
    """è©•ä¾¡çµæœã®ä¿å­˜"""
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

def interactive_evaluation(results: Dict, sample_size: int = 20):
    """
    ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªè©•ä¾¡
    ã‚µãƒ³ãƒ—ãƒ«ã®è³ªå•ã«å¯¾ã—ã¦å‚ç…§å›ç­”ã‚’å…¥åŠ›ã—ã¦ã‚‚ã‚‰ã„ã€è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—
    
    Args:
        results: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
        sample_size: è©•ä¾¡ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°
    """
    evaluator = RAGEvaluator()
    detailed_results = results.get('detailed_results', [])
    
    print("\n" + "="*60)
    print("Post Evaluation - å›ç­”å“è³ªã®è©•ä¾¡")
    print("="*60)
    print(f"\nç·è³ªå•æ•°: {len(detailed_results)}")
    print(f"è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«æ•°: {sample_size}å•")
    print("\nå„è³ªå•ã«å¯¾ã—ã¦ã€æ­£ã—ã„å‚ç…§å›ç­”ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    print("ï¼ˆã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹å ´åˆã¯ Enter ã‚­ãƒ¼ã‚’æŠ¼ã—ã¦ãã ã•ã„ï¼‰\n")
    
    evaluated_count = 0
    evaluation_scores = {
        'exact_match': [],
        'f1_score': [],
        'bleu_1': [],
        'bleu_2': [],
        'rouge1_f': [],
        'rougeL_f': []
    }
    
    # ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«å‡ç­‰ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    categories = {}
    for item in detailed_results:
        cat = item.get('category', 'unknown')
        if cat not in categories:
            categories[cat] = []
        categories[cat].append(item)
    
    # å„ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰å‡ç­‰ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    samples = []
    per_category = max(1, sample_size // len(categories))
    for cat, items in categories.items():
        samples.extend(items[:per_category])
    samples = samples[:sample_size]
    
    for i, item in enumerate(samples, 1):
        print(f"\n{'='*60}")
        print(f"è³ªå• {i}/{len(samples)}")
        print(f"ã‚«ãƒ†ã‚´ãƒª: {item.get('category', 'unknown')}")
        print(f"{'='*60}")
        print(f"\nè³ªå•: {item['question']}")
        print(f"\nRAGå›ç­”:\n{item['response'][:300]}...")
        
        reference = input("\nâœï¸  æ­£ã—ã„å‚ç…§å›ç­”ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆEnter ã§ã‚¹ã‚­ãƒƒãƒ—ï¼‰: ").strip()
        
        if not reference:
            print("â­ï¸  ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
            continue
        
        # è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—
        eval_result = evaluator.evaluate(
            item['response'],
            reference,
            item.get('response_time'),
            item.get('memory_usage_mb')
        )
        
        # çµæœã‚’ä¿å­˜
        item['reference'] = reference
        item['evaluation'] = eval_result
        
        # ã‚¹ã‚³ã‚¢ã‚’é›†è¨ˆ
        for key in evaluation_scores:
            if key in eval_result:
                evaluation_scores[key].append(eval_result[key])
        
        evaluated_count += 1
        
        # è©•ä¾¡çµæœã®è¡¨ç¤º
        print(f"\nğŸ“Š è©•ä¾¡çµæœ:")
        print(f"  EM: {eval_result.get('exact_match', 0):.2f}")
        print(f"  F1: {eval_result.get('f1_score', 0):.2f}")
        print(f"  BLEU-1: {eval_result.get('bleu_1', 0):.2f}")
        print(f"  ROUGE-1: {eval_result.get('rouge1_f', 0):.2f}")
    
    # é›†è¨ˆçµæœã®è¨ˆç®—
    print(f"\n\n{'='*60}")
    print("è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼")
    print(f"{'='*60}")
    print(f"è©•ä¾¡å®Œäº†æ•°: {evaluated_count}/{len(samples)}")
    
    if evaluated_count > 0:
        print(f"\nå¹³å‡ã‚¹ã‚³ã‚¢:")
        for metric, values in evaluation_scores.items():
            if values:
                avg = sum(values) / len(values)
                print(f"  {metric}: {avg:.3f}")
        
        # çµæœã«çµ±è¨ˆæƒ…å ±ã‚’è¿½åŠ 
        results['evaluation_statistics'] = {
            'evaluated_count': evaluated_count,
            'total_samples': len(samples),
            'metrics': {
                metric: {
                    'mean': sum(values) / len(values) if values else 0,
                    'min': min(values) if values else 0,
                    'max': max(values) if values else 0
                }
                for metric, values in evaluation_scores.items() if values
            }
        }
    
    return results

def llm_judge_evaluation(results: Dict, sample_size: int = 20):
    """
    LLM-as-a-Judgeè©•ä¾¡
    åˆ¥ã®LLMã‚’ä½¿ã£ã¦å›ç­”ã®å“è³ªã‚’è©•ä¾¡
    
    Args:
        results: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
        sample_size: è©•ä¾¡ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°
    """
    print("\nâš ï¸  LLM-as-a-Judgeè©•ä¾¡ã¯æœªå®Ÿè£…ã§ã™ã€‚")
    print("å°†æ¥çš„ã«ã¯ã€åˆ¥ã®LLMã‚’ä½¿ã£ã¦è‡ªå‹•è©•ä¾¡ã‚’è¡Œã†æ©Ÿèƒ½ã‚’è¿½åŠ äºˆå®šã§ã™ã€‚")
    return results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®äº‹å¾Œè©•ä¾¡')
    parser.add_argument('--input', '-i', required=True, help='ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœJSONãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('--output', '-o', help='è©•ä¾¡çµæœã®å‡ºåŠ›å…ˆï¼ˆçœç•¥æ™‚ã¯å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸Šæ›¸ãï¼‰')
    parser.add_argument('--sample-size', '-n', type=int, default=20, help='è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 20ï¼‰')
    parser.add_argument('--mode', '-m', choices=['interactive', 'llm-judge'], default='interactive',
                        help='è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ï¼ˆinteractive: æ‰‹å‹•å…¥åŠ›ã€llm-judge: LLMè‡ªå‹•è©•ä¾¡ï¼‰')
    
    args = parser.parse_args()
    
    # çµæœã®èª­ã¿è¾¼ã¿
    print(f"\nğŸ“‚ èª­ã¿è¾¼ã¿ä¸­: {args.input}")
    results = load_benchmark_results(args.input)
    
    # è©•ä¾¡ã®å®Ÿè¡Œ
    if args.mode == 'interactive':
        results = interactive_evaluation(results, args.sample_size)
    elif args.mode == 'llm-judge':
        results = llm_judge_evaluation(results, args.sample_size)
    
    # çµæœã®ä¿å­˜
    output_path = args.output or args.input
    print(f"\nğŸ’¾ ä¿å­˜ä¸­: {output_path}")
    save_evaluation_results(output_path, results)
    
    print("\nâœ… è©•ä¾¡å®Œäº†ï¼")
    
    # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
    if 'evaluation_statistics' in results:
        stats = results['evaluation_statistics']
        print(f"\næœ€çµ‚çµ±è¨ˆ:")
        print(f"  è©•ä¾¡æ¸ˆã¿: {stats['evaluated_count']}/{stats['total_samples']}å•")
        if stats['metrics']:
            print(f"\n  å¹³å‡ã‚¹ã‚³ã‚¢:")
            for metric, values in stats['metrics'].items():
                print(f"    {metric}: {values['mean']:.3f}")

if __name__ == "__main__":
    main()
