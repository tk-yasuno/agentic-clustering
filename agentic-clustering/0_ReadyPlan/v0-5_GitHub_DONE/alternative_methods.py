# -*- coding: utf-8 -*-
"""
ä»£æ›¿ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«: Agentic Clustering v0.2
GMM, DBSCANãªã©ã®ä»£æ›¿ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æä¾›
"""

import numpy as np
from sklearn.mixture import GaussianMixture
from sklearn.cluster import DBSCAN, KMeans
from sklearn.metrics import silhouette_score
import config

class AlternativeClusteringMethods:
    """ä»£æ›¿ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•ã‚’æä¾›ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, X_scaled):
        """
        Parameters:
        -----------
        X_scaled : array-like
            æ¨™æº–åŒ–ã•ã‚ŒãŸç‰¹å¾´é‡
        """
        self.X_scaled = X_scaled
        self.results = {}
    
    def try_kmeans(self, n_clusters):
        """KMeansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°"""
        print(f"\nðŸ”µ KMeans (k={n_clusters}) ã‚’å®Ÿè¡Œä¸­...")
        
        kmeans = KMeans(
            n_clusters=n_clusters,
            random_state=config.RANDOM_STATE,
            n_init=10,
            max_iter=300
        )
        
        labels = kmeans.fit_predict(self.X_scaled)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒã‚’è¡¨ç¤º
        unique, counts = np.unique(labels, return_counts=True)
        print(f"   ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {len(unique)}")
        for cluster_id, count in zip(unique, counts):
            print(f"   ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id}: {count}ä»¶")
        
        self.results['KMeans'] = {
            'model': kmeans,
            'labels': labels,
            'n_clusters': n_clusters
        }
        
        return labels
    
    def try_gmm(self, n_components_range=None):
        """ã‚¬ã‚¦ã‚¹æ··åˆãƒ¢ãƒ‡ãƒ«ï¼ˆGMMï¼‰"""
        print(f"\nðŸŸ£ GMM (Gaussian Mixture Model) ã‚’å®Ÿè¡Œä¸­...")
        
        if n_components_range is None:
            n_components_range = range(config.MIN_CLUSTERS, config.MAX_CLUSTERS + 1)
        
        best_gmm = None
        best_labels = None
        best_score = -1
        best_n = config.MIN_CLUSTERS
        
        for n in n_components_range:
            gmm = GaussianMixture(
                n_components=n,
                covariance_type='full',
                random_state=config.RANDOM_STATE,
                n_init=10
            )
            
            labels = gmm.fit_predict(self.X_scaled)
            
            # ãƒŽã‚¤ã‚ºã‚¯ãƒ©ã‚¹ã‚¿ï¼ˆ-1ï¼‰ãŒã‚ã‚‹å ´åˆã¯é™¤å¤–ã—ã¦è©•ä¾¡
            if len(np.unique(labels)) > 1:
                score = silhouette_score(self.X_scaled, labels)
                print(f"   n_components={n}: ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ = {score:.4f}")
                
                if score > best_score:
                    best_score = score
                    best_gmm = gmm
                    best_labels = labels
                    best_n = n
        
        print(f"   âœ“ æœ€é©ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ•°: {best_n} (ã‚¹ã‚³ã‚¢: {best_score:.4f})")
        
        # ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒã‚’è¡¨ç¤º
        unique, counts = np.unique(best_labels, return_counts=True)
        print(f"   ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {len(unique)}")
        for cluster_id, count in zip(unique, counts):
            print(f"   ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id}: {count}ä»¶")
        
        self.results['GMM'] = {
            'model': best_gmm,
            'labels': best_labels,
            'n_clusters': best_n,
            'score': best_score
        }
        
        return best_labels
    
    def try_dbscan(self, eps_range=None, min_samples_range=None, target_clusters=50):
        """DBSCAN(å¯†åº¦ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°)"""
        print(f"\nðŸŸ¢ DBSCAN (Density-Based Spatial Clustering) ã‚’å®Ÿè¡Œä¸­...")
        print(f"   ç›®æ¨™ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {target_clusters}ç¨‹åº¦")
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¯„å›²ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿æ•°50ç¨‹åº¦ã«èª¿æ•´ï¼‰
        if eps_range is None:
            eps_range = [0.8, 1.0, 1.2, 1.4, 1.6]
        
        if min_samples_range is None:
            min_samples_range = [15, 20, 25, 30, 35]
        
        best_dbscan = None
        best_labels = None
        best_score = -1
        best_params = None
        
        for eps in eps_range:
            for min_samples in min_samples_range:
                dbscan = DBSCAN(eps=eps, min_samples=min_samples)
                labels = dbscan.fit_predict(self.X_scaled)
                
                # ãƒŽã‚¤ã‚ºãƒã‚¤ãƒ³ãƒˆï¼ˆ-1ï¼‰ã‚’é™¤ã„ãŸã‚¯ãƒ©ã‚¹ã‚¿æ•°
                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                n_noise = list(labels).count(-1)
                
                # ã‚¯ãƒ©ã‚¹ã‚¿ãŒ2ã¤ä»¥ä¸Šã‚ã‚‹å ´åˆã®ã¿è©•ä¾¡
                if n_clusters >= 2:
                    # ãƒŽã‚¤ã‚ºãƒã‚¤ãƒ³ãƒˆã‚’é™¤å¤–ã—ã¦ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                    mask = labels != -1
                    if mask.sum() > 0:
                        score = silhouette_score(self.X_scaled[mask], labels[mask])
                        
                        # ç›®æ¨™ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‹ã‚‰ã®è·é›¢ã‚’è¨ˆç®—ï¼ˆãƒšãƒŠãƒ«ãƒ†ã‚£ï¼‰
                        cluster_penalty = abs(n_clusters - target_clusters) / target_clusters
                        adjusted_score = score * (1 - cluster_penalty * 0.5)  # ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã®å·®ã«å¿œã˜ã¦ã‚¹ã‚³ã‚¢ã‚’èª¿æ•´
                        
                        print(f"   eps={eps}, min_samples={min_samples}: "
                              f"ã‚¯ãƒ©ã‚¹ã‚¿æ•°={n_clusters}, ãƒŽã‚¤ã‚º={n_noise}, "
                              f"ã‚¹ã‚³ã‚¢={score:.4f}, èª¿æ•´å¾Œ={adjusted_score:.4f}")
                        
                        # ãƒŽã‚¤ã‚ºãŒå°‘ãªãã€èª¿æ•´å¾Œã‚¹ã‚³ã‚¢ãŒé«˜ãã€ã‚¯ãƒ©ã‚¹ã‚¿æ•°ãŒç›®æ¨™ã«è¿‘ã„ã‚‚ã®ã‚’é¸æŠž
                        if (adjusted_score > best_score and 
                            n_noise < len(labels) * 0.35 and 
                            20 <= n_clusters <= 100):  # ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã®å¦¥å½“ãªç¯„å›²ï¼ˆ60ã‚’ä¸­å¿ƒã«ï¼‰
                            best_score = adjusted_score
                            best_dbscan = dbscan
                            best_labels = labels
                            best_params = {'eps': eps, 'min_samples': min_samples}
        
        if best_labels is not None:
            n_clusters_final = len(set(best_labels)) - (1 if -1 in best_labels else 0)
            n_noise_final = list(best_labels).count(-1)
            
            print(f"   âœ“ æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: eps={best_params['eps']}, "
                  f"min_samples={best_params['min_samples']} (èª¿æ•´å¾Œã‚¹ã‚³ã‚¢: {best_score:.4f})")
            print(f"   âœ“ ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {n_clusters_final}, ãƒŽã‚¤ã‚º: {n_noise_final}ä»¶")
            
            # ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒã‚’è¡¨ç¤ºï¼ˆä¸Šä½10ä»¶ã®ã¿ï¼‰
            unique, counts = np.unique(best_labels, return_counts=True)
            sorted_clusters = sorted(zip(unique, counts), key=lambda x: x[1], reverse=True)
            print(f"   ä¸»è¦ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒï¼ˆä¸Šä½10ä»¶ï¼‰:")
            for i, (cluster_id, count) in enumerate(sorted_clusters[:10]):
                label_name = "ãƒŽã‚¤ã‚º" if cluster_id == -1 else f"ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id}"
                print(f"     {label_name}: {count}ä»¶")
            
            self.results['DBSCAN'] = {
                'model': best_dbscan,
                'labels': best_labels,
                'n_clusters': len(set(best_labels)) - (1 if -1 in best_labels else 0),
                'score': best_score,
                'params': best_params
            }
        else:
            print(f"   âš ï¸ é©åˆ‡ãªDBSCANãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å®Ÿè¡Œ
            dbscan = DBSCAN(eps=0.5, min_samples=5)
            best_labels = dbscan.fit_predict(self.X_scaled)
            
            self.results['DBSCAN'] = {
                'model': dbscan,
                'labels': best_labels,
                'n_clusters': len(set(best_labels)) - (1 if -1 in best_labels else 0),
                'score': -1,
                'params': {'eps': 0.5, 'min_samples': 5}
            }
        
        return best_labels
    
    def try_hdbscan(self, min_cluster_size_range=None, min_samples_range=None, target_clusters=50):
        """HDBSCAN (Hierarchical Density-Based Spatial Clustering)
        
        DBSCANã®ã‚¯ãƒ©ã‚¹ã‚¿æ•°ãŒå¤šã™ãŽã‚‹å ´åˆã®ä»£æ›¿æ‰‹æ³•ã¨ã—ã¦ä½¿ç”¨ã€‚
        HDBSCANã¯éšŽå±¤çš„ãªå¯†åº¦ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã§ã€ã‚ˆã‚Šé©å¿œçš„ãªã‚¯ãƒ©ã‚¹ã‚¿ã‚’ç”Ÿæˆã™ã‚‹ã€‚
        """
        print(f"\nðŸŸ¡ HDBSCAN (Hierarchical DBSCAN) ã‚’å®Ÿè¡Œä¸­...")
        print(f"   ç›®æ¨™ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {target_clusters}ç¨‹åº¦")
        
        try:
            import hdbscan
        except ImportError:
            print(f"   âš ï¸ HDBSCANãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            print(f"   'pip install hdbscan' ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
            return None
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¯„å›²
        if min_cluster_size_range is None:
            min_cluster_size_range = [10, 15, 20, 30, 40]  # ã‚ˆã‚Šç´°ã‹ã„ã‚¯ãƒ©ã‚¹ã‚¿ã‚’ç”Ÿæˆ
        
        if min_samples_range is None:
            min_samples_range = [5, 8, 10]  # min_samplesã‚‚å°ã•ãèª¿æ•´
        
        best_hdbscan = None
        best_labels = None
        best_score = -1
        best_params = None
        
        for min_cluster_size in min_cluster_size_range:
            for min_samples in min_samples_range:
                try:
                    clusterer = hdbscan.HDBSCAN(
                        min_cluster_size=min_cluster_size,
                        min_samples=min_samples,
                        cluster_selection_method='eom',  # Excess of Mass
                        metric='euclidean'
                    )
                    
                    labels = clusterer.fit_predict(self.X_scaled)
                    
                    # ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã¨ãƒŽã‚¤ã‚ºãƒã‚¤ãƒ³ãƒˆæ•°
                    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                    n_noise = list(labels).count(-1)
                    
                    # ã‚¯ãƒ©ã‚¹ã‚¿ãŒ2ã¤ä»¥ä¸Šã‚ã‚‹å ´åˆã®ã¿è©•ä¾¡
                    if n_clusters >= 2:
                        # ãƒŽã‚¤ã‚ºãƒã‚¤ãƒ³ãƒˆã‚’é™¤å¤–ã—ã¦ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                        mask = labels != -1
                        if mask.sum() > 1 and len(set(labels[mask])) > 1:
                            score = silhouette_score(self.X_scaled[mask], labels[mask])
                            
                            # ç›®æ¨™ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‹ã‚‰ã®è·é›¢ã‚’è¨ˆç®—(ãƒšãƒŠãƒ«ãƒ†ã‚£)
                            cluster_penalty = abs(n_clusters - target_clusters) / target_clusters
                            # ã‚¯ãƒ©ã‚¹ã‚¿æ•°ãŒç›®æ¨™ã«è¿‘ã„ã»ã©é«˜ã‚¹ã‚³ã‚¢ã€ãƒŽã‚¤ã‚ºæ¯”çŽ‡ã‚‚è€ƒæ…®
                            noise_penalty = n_noise / len(labels)
                            adjusted_score = score * (1 - cluster_penalty * 0.5) * (1 - noise_penalty * 0.3)
                            
                            print(f"   min_cluster_size={min_cluster_size}, min_samples={min_samples}: "
                                  f"ã‚¯ãƒ©ã‚¹ã‚¿æ•°={n_clusters}, ãƒŽã‚¤ã‚º={n_noise}, "
                                  f"ã‚¹ã‚³ã‚¢={score:.4f}, èª¿æ•´å¾Œ={adjusted_score:.4f}")
                            
                            # ãƒŽã‚¤ã‚ºãŒå°‘ãªãã€èª¿æ•´å¾Œã‚¹ã‚³ã‚¢ãŒé«˜ãã€ã‚¯ãƒ©ã‚¹ã‚¿æ•°ãŒé©åˆ‡ãªã‚‚ã®ã‚’é¸æŠž
                            # å®Ÿç”¨çš„ãªã‚¯ãƒ©ã‚¹ã‚¿æ•°ç¯„å›²: 20ã€œ80å€‹(æ„æ€æ±ºå®šã«é©ã—ãŸç²’åº¦)
                            # ãƒŽã‚¤ã‚ºæ¯”çŽ‡: 40%ä»¥ä¸‹
                            if (adjusted_score > best_score and 
                                n_noise < len(labels) * 0.40 and 
                                20 <= n_clusters <= 80):
                                best_score = adjusted_score
                                best_hdbscan = clusterer
                                best_labels = labels
                                best_params = {'min_cluster_size': min_cluster_size, 'min_samples': min_samples}
                
                except Exception as e:
                    print(f"   âš ï¸ min_cluster_size={min_cluster_size}, min_samples={min_samples}ã§ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
        
        if best_labels is not None:
            n_clusters_final = len(set(best_labels)) - (1 if -1 in best_labels else 0)
            n_noise_final = list(best_labels).count(-1)
            
            print(f"   âœ“ æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: min_cluster_size={best_params['min_cluster_size']}, "
                  f"min_samples={best_params['min_samples']} (èª¿æ•´å¾Œã‚¹ã‚³ã‚¢: {best_score:.4f})")
            print(f"   âœ“ ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {n_clusters_final}, ãƒŽã‚¤ã‚º: {n_noise_final}ä»¶")
            
            # ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒã‚’è¡¨ç¤ºï¼ˆä¸Šä½10ä»¶ã®ã¿ï¼‰
            unique, counts = np.unique(best_labels, return_counts=True)
            sorted_clusters = sorted(zip(unique, counts), key=lambda x: x[1], reverse=True)
            print(f"   ä¸»è¦ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒï¼ˆä¸Šä½10ä»¶ï¼‰:")
            for i, (cluster_id, count) in enumerate(sorted_clusters[:10]):
                label_name = "ãƒŽã‚¤ã‚º" if cluster_id == -1 else f"ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id}"
                print(f"     {label_name}: {count}ä»¶")
            
            self.results['HDBSCAN'] = {
                'model': best_hdbscan,
                'labels': best_labels,
                'n_clusters': n_clusters_final,
                'score': best_score,
                'params': best_params
            }
        else:
            print(f"   âš ï¸ é©åˆ‡ãªHDBSCANãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            self.results['HDBSCAN'] = None
        
        return best_labels
    
    def get_results(self):
        """ã™ã¹ã¦ã®çµæžœã‚’è¿”ã™"""
        return self.results


class AlternativeDimensionalityReduction:
    """ä»£æ›¿æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•ã‚’æä¾›ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, X_scaled):
        """
        Parameters:
        -----------
        X_scaled : array-like
            æ¨™æº–åŒ–ã•ã‚ŒãŸç‰¹å¾´é‡
        """
        self.X_scaled = X_scaled
        self.results = {}
    
    def try_tsne(self, n_components=2, perplexity_range=None):
        """t-SNEï¼ˆt-distributed Stochastic Neighbor Embeddingï¼‰"""
        print(f"\nðŸ”´ t-SNE ã‚’å®Ÿè¡Œä¸­...")
        
        from sklearn.manifold import TSNE
        
        if perplexity_range is None:
            # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«å¿œã˜ã¦é©åˆ‡ãªperplexityã‚’é¸æŠž
            n_samples = len(self.X_scaled)
            perplexity_range = [min(30, n_samples // 4), 
                               min(50, n_samples // 3)]
        
        best_tsne = None
        best_embedding = None
        best_perplexity = perplexity_range[0]
        
        for perplexity in perplexity_range:
            try:
                # scikit-learnã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«å¿œã˜ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´
                tsne_params = {
                    'n_components': n_components,
                    'perplexity': perplexity,
                    'random_state': config.RANDOM_STATE
                }
                
                # ãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›æ€§ã®ãŸã‚ã€max_iterã¨n_iterã®ä¸¡æ–¹ã‚’è©¦ã™
                try:
                    tsne = TSNE(**tsne_params, n_iter=1000, n_iter_without_progress=300)
                except TypeError:
                    tsne = TSNE(**tsne_params, max_iter=1000, n_iter_without_progress=300)
                
                embedding = tsne.fit_transform(self.X_scaled)
                
                # KL divergenceãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯è¡¨ç¤º
                if hasattr(tsne, 'kl_divergence_'):
                    print(f"   perplexity={perplexity}: KL divergence = {tsne.kl_divergence_:.4f}")
                    if best_tsne is None or tsne.kl_divergence_ < best_tsne.kl_divergence_:
                        best_tsne = tsne
                        best_embedding = embedding
                        best_perplexity = perplexity
                else:
                    print(f"   perplexity={perplexity}: å®Œäº†")
                    if best_tsne is None:
                        best_tsne = tsne
                        best_embedding = embedding
                        best_perplexity = perplexity
            
            except Exception as e:
                print(f"   âš ï¸ perplexity={perplexity}ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        if best_embedding is not None:
            print(f"   âœ“ æœ€é©perplexity: {best_perplexity}")
            
            self.results['t-SNE'] = {
                'model': best_tsne,
                'embedding': best_embedding,
                'perplexity': best_perplexity
            }
        else:
            print(f"   âš ï¸ t-SNEã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            self.results['t-SNE'] = None
        
        return best_embedding
    
    def try_umap(self, n_components=2, n_neighbors_range=None):
        """UMAPï¼ˆUniform Manifold Approximation and Projectionï¼‰"""
        print(f"\nðŸŸ  UMAP ã‚’å®Ÿè¡Œä¸­...")
        
        try:
            import umap
        except ImportError:
            print(f"   âš ï¸ UMAPãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            print(f"   'pip install umap-learn' ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
            self.results['UMAP'] = None
            return None
        
        if n_neighbors_range is None:
            n_samples = len(self.X_scaled)
            n_neighbors_range = [min(15, n_samples // 10),
                                 min(30, n_samples // 5)]
        
        best_umap = None
        best_embedding = None
        best_n_neighbors = n_neighbors_range[0]
        
        for n_neighbors in n_neighbors_range:
            try:
                umap_model = umap.UMAP(
                    n_components=n_components,
                    n_neighbors=n_neighbors,
                    random_state=config.RANDOM_STATE,
                    min_dist=0.1
                )
                
                embedding = umap_model.fit_transform(self.X_scaled)
                
                print(f"   n_neighbors={n_neighbors}: å®Œäº†")
                
                if best_umap is None:
                    best_umap = umap_model
                    best_embedding = embedding
                    best_n_neighbors = n_neighbors
            
            except Exception as e:
                print(f"   âš ï¸ n_neighbors={n_neighbors}ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        if best_embedding is not None:
            print(f"   âœ“ æœ€é©n_neighbors: {best_n_neighbors}")
            
            self.results['UMAP'] = {
                'model': best_umap,
                'embedding': best_embedding,
                'n_neighbors': best_n_neighbors
            }
        else:
            print(f"   âš ï¸ UMAPã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            self.results['UMAP'] = None
        
        return best_embedding
    
    def get_results(self):
        """ã™ã¹ã¦ã®çµæžœã‚’è¿”ã™"""
        return self.results
