# Benchmark Lessons - Gemma3 RAG KasenSabo MVP

**実行日**: 2025年11月19日  
**環境**: NVIDIA GeForce RTX 4060 Ti (16GB), Windows, Python 3.12

---

## 📊 ベンチマーク結果サマリー

### 実行構成
- **質問数**: 200問（10カテゴリ × 各20問）
- **モデル**: Gemma 3 (2B) - INT4 (q4_K_M) vs INT8 (q8_0)
- **ベクトルDB**: FAISS（768次元、6,471チャンク）
- **埋め込みモデル**: intfloat/multilingual-e5-base
- **実行時間**: 約7分（各モデル）

### パフォーマンス比較

| 指標 | INT4 (q4_K_M) | INT8 (q8_0) | 差分 |
|------|---------------|-------------|------|
| **平均応答時間** | 0.77秒 | 1.21秒 | INT4が **36%高速** ⭐ |
| **応答時間中央値** | 0.65秒 | 0.74秒 | INT4が 12%高速 |
| **標準偏差** | 0.31秒 | 4.02秒 | INT4が **13倍安定** ⭐⭐ |
| **最速応答** | 0.45秒 | 0.50秒 | ほぼ同等 |
| **最遅応答** | 2.25秒 | 57.25秒 | INT8に異常値あり ⚠️ |
| **GPUメモリ** | 約2.5GB | 約3-4GB | INT4がメモリ効率良 |

---

## 🎯 主要な発見

### 1. **INT4の予想外の優位性**

**期待していたこと:**
- INT8の方が精度が高く、品質重視のシナリオで有利

**実際の結果:**
- INT4が **速度・安定性の両面で優秀**
- 応答時間が36%短く、標準偏差が13倍小さい
- 最遅応答でも2.25秒以内に収まる

**教訓:**
> 量子化レベルが高い（INT4）方が必ずしも品質が低いわけではない。小型モデル（2B）では、INT4の方が推論が安定し、実用的。

### 2. **INT8の異常な応答遅延**

**観測された問題:**
- 「原則・考え方の説明」カテゴリで平均3.61秒（INT4は0.72秒）
- 最遅ケースで57.25秒の異常値
- 標準偏差が4.02秒と非常に大きい

**推測される原因:**
1. INT8モデルのメモリアクセスパターンが非効率
2. 複雑な抽象概念の処理でトークン生成が不安定
3. GPU上でのINT8演算の最適化不足

**教訓:**
> 高精度量子化（INT8）でも、小型モデルでは推論の不安定性が顕著。カテゴリによって極端な性能差が出る可能性がある。

### 3. **カテゴリ別パフォーマンス傾向**

#### 🚀 高速カテゴリ（両モデル共通）
- **材料規格・仕様**: 0.61秒 (INT4) / 0.73秒 (INT8)
- **寸法・サイズ規定**: 0.62秒 / 0.69秒
- **計算式・係数**: 0.67秒 / 0.86秒

**特徴**: 具体的・定量的な質問は高速

#### 🐢 低速カテゴリ
- **比較・関係性の説明**: 1.09秒 (INT4) / 1.21秒 (INT8)
- **全体構造・体系の理解**: 0.93秒 / 1.19秒
- **原則・考え方の説明**: 0.72秒 / **3.61秒** ⚠️

**特徴**: 抽象的・概念的な質問は処理時間が長い

**教訓:**
> RAGシステムのベンチマークでは、質問タイプ（具体的 vs 抽象的）による性能差を必ず測定すべき。

---

## 🔧 技術的教訓

### 1. **FAISS vs ChromaDB**

**当初の計画**: ChromaDB使用  
**実際の選択**: FAISS

**理由:**
- ChromaDBはC++コンパイラ依存（chroma-hnswlib）
- Windowsで Visual C++ 14.0以上が必要
- FAISSはpre-builtバイナリで即座に動作

**教訓:**
> Windowsでの開発では、コンパイラ依存の少ないライブラリを優先すべき。FAISSは移植性が高く推奨。

### 2. **埋め込みモデルの選択**

**試行錯誤:**
1. multilingual-e5-large（2.5GB）→ ダウンロード遅い
2. multilingual-e5-small（470MB）→ 次元数不足懸念
3. **multilingual-e5-base（1.1GB、768次元）** → ✅ 採用

**理由:**
- ダウンロード時間とモデルサイズのバランス
- 768次元で十分な精度
- 日本語対応が良好

**教訓:**
> 大規模モデルよりも、中間サイズ（base）の方がダウンロード・実行の両面でバランスが良い。

### 3. **hf_transferの効果**

**期待**: HuggingFaceモデルの高速ダウンロード  
**実際**: 効果が限定的

**問題点:**
- 環境変数設定だけでは不十分な場合あり
- huggingface_hubのバージョン依存性が高い
- パッケージの依存関係競合が発生

**教訓:**
> hf_transferは有効だが、依存関係の事前確認が必須。ダウンロード速度改善より、適切なモデルサイズ選択の方が効果的。

### 4. **仮想環境 vs グローバル環境**

**試行錯誤:**
- 複数の仮想環境でPyTorch DLLエラー
- グローバルPython 3.12で安定動作

**原因:**
- 仮想環境内の古いllama-indexバージョン
- PyTorchとCUDAの依存関係の複雑さ

**教訓:**
> 深層学習環境では、仮想環境が必ずしも最適ではない。グローバル環境でパッケージを統一管理する方が安定する場合がある。

---

## 💡 システム設計の教訓

### 1. **システムプロンプトの重要性**

**問題**: 初期実行で英語回答が多発

**解決策:**
```python
system_prompt="あなたは日本の河川・ダム・砂防技術の専門家です。
質問に対して、提供されたコンテキスト情報に基づいて、
日本語で正確かつ簡潔に回答してください。"
```

**効果**: 日本語回答率が大幅向上

**教訓:**
> 多言語LLMでは、明示的な言語指定のシステムプロンプトが必須。特に技術文書では専門家ペルソナの設定が有効。

### 2. **チャンクサイズとオーバーラップ**

**採用設定:**
- チャンクサイズ: 512トークン
- オーバーラップ: 50トークン

**結果:**
- 8ドキュメントから6,471チャンク生成
- 検索精度とインデックスサイズのバランス良好

**教訓:**
> 技術文書では512トークンが適切。小さすぎると文脈が失われ、大きすぎると検索精度が低下。

### 3. **GPU推論の効果**

**CPU vs GPU:**
- CPU: 初期テストで遅延大
- GPU (RTX 4060 Ti): 0.77秒/問（INT4）

**GPU使用率:**
- 推論中: 2.5GB VRAM使用
- アイドル時: 0GB

**教訓:**
> 小型LLM（2B）でもGPU推論は必須。CPUでは実用的な速度が得られない。

### 4. **進捗バーの重要性**

**問題**: tqdmの進捗バーが重複表示

**解決策:**
```python
tqdm(questions, desc="Progress", ncols=100, leave=True, position=0)
```

**効果**: クリーンな進捗表示

**教訓:**
> 長時間実行のベンチマークでは、ユーザー体験のための進捗表示が重要。`ncols`と`position`の明示的設定が推奨。

---

## 📈 ベンチマーク設計の教訓

### 1. **中間保存の実装**

**実装内容:**
- 50問ごとに結果を自動保存
- Ctrl+Cで中断しても結果を保持

**効果:**
- 長時間実行の安心感
- 障害時のデータ損失を最小化

**教訓:**
> 200問以上のベンチマークでは、中間保存が必須。save_intervalは全体の25%程度が適切。

### 2. **カテゴリ別分析**

**実装内容:**
- 10カテゴリで質問を分類
- カテゴリ別に平均応答時間を計算

**発見:**
- カテゴリによって最大5倍の性能差
- 抽象概念の処理が特に遅い

**教訓:**
> 単純な平均だけでなく、カテゴリ別分析が必須。ユースケース別の性能評価が重要。

### 3. **異常値の検出**

**発見:**
- INT8で57.25秒の異常値
- 標準偏差が4.02秒と極端に大きい

**対応:**
- 中央値も併記して実態を把握
- 最大値・最小値の記録

**教訓:**
> 平均値だけでなく、中央値・標準偏差・最大値を必ず記録。異常値の存在はシステムの不安定性を示す重要な指標。

---

## 🎯 推奨事項

### プロダクション環境での選択

#### ✅ INT4 (q4_K_M) を推奨するケース
- **リアルタイム応答が必要**: 平均0.77秒、最遅2.25秒
- **安定性重視**: 標準偏差0.31秒と安定
- **メモリ制約**: 2.5GB VRAMで動作
- **コスト重視**: 推論コスト削減

#### ⚠️ INT8 (q8_0) を検討するケース
- **精度が最優先**: 理論上は高精度（実測では差が小さい）
- **異常値許容可能**: 時々遅延しても問題ない
- **バッチ処理**: リアルタイム性不要

### 総合推奨
> **INT4 (q4_K_M) を標準として採用すべき**
> - 速度・安定性・メモリ効率すべてで優位
> - 小型モデル（2B）では量子化による品質低下が限定的
> - 実用的なレイテンシ要件を満たす

---

## 🔍 今後の検証項目

### 1. 精度評価の追加
- **現状**: 応答時間のみ測定
- **必要**: EM、F1、BLEU、ROUGEスコア算出
- **理由**: 速度だけでなく回答品質の定量評価

### 2. より大きなモデルでの検証
- Gemma 7B (INT4 vs INT8)
- モデルサイズによる量子化の影響を確認

### 3. 温度パラメータの最適化
- **現状**: temperature=0.1（固定）
- **検証**: 0.0〜0.5での応答品質変化

### 4. Top-Kパラメータの影響
- **現状**: similarity_top_k=3（固定）
- **検証**: 1, 3, 5での精度・速度トレードオフ

### 5. 埋め込みモデルの比較
- multilingual-e5-base vs large
- 次元数（768 vs 1024）の影響評価

---

## 📚 参考資料

### 生成ファイル
- `results/model_comparison_20251119_204111.csv` - モデル比較結果
- `results/gemma_2b-instruct-q4_K_M_statistics_20251119_203701.csv` - INT4統計
- `results/gemma_2b-instruct-q8_0_statistics_20251119_204111.csv` - INT8統計
- `results/*_benchmark_*.json` - 全質問の詳細結果

### 技術スタック
- Ollama 0.12.11
- LlamaIndex 0.3.79
- FAISS (faiss-cpu)
- PyTorch 2.x + CUDA 12.9

---

## 🎓 最終的な学び

1. **量子化は単純な精度トレードオフではない**: INT4が総合的に優秀だった
2. **安定性が速度と同じくらい重要**: 標準偏差を必ず測定すべき
3. **カテゴリ別分析は必須**: 平均値だけでは不十分
4. **環境構築の複雑さを過小評価しない**: FAISS選択が正解だった
5. **日本語LLMでもプロンプトは重要**: システムプロンプトで品質向上
6. **GPU推論は小型モデルでも必須**: CPUでは実用に耐えない
7. **中間保存で安心感**: 長時間実行の実装必須項目
8. **異常値検出が重要**: 最大値・標準偏差の確認を忘れない

---

**作成日**: 2025年11月19日  
**作成者**: Gemma3 RAG KasenSabo MVP Team  
**ライセンス**: MIT License
